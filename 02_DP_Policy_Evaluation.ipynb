{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62e1258",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "- Can be used to \"plan\" i.e. $\\pi^*$ or $V^*$ find when MDP $(S, P, d, R, A)$ is fully known\n",
    "    - 1) Policy Evaluation / Prediction - For any $\\pi$ find $V^\\pi(s)$ and $Q^\\pi(s,a)$ \n",
    "    - 2) Policy Iteration / Control - Find $\\pi^*$ by $\\pi_{k+1}(s) = argmax_{a} Q^{\\pi_{k}}(s,a)$\n",
    "    - 3) Value Iteration / Control -  Find $V^*$ direclty via $V_{k+1}(s) = max_{a} \\left( R^a_{s} + d * \\sum_{s'}P^a_{s,s'}V(s') \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8270d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (30, 10)\n",
    "plt.rcParams['font.size']=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56eeae",
   "metadata": {},
   "source": [
    "# 1) Policy Evaluation\n",
    "\n",
    "\n",
    "- Given a MDP $(S, P, d, R, A)$ for any arbitrary policy $\\pi$ we can find $V^\\pi$\n",
    "- Guess $V^\\pi_{0}$, and calculate $V^\\pi_{1}$, $V^\\pi_{2}$, ... \n",
    "- for each state $s$, $V^\\pi_{k+1}(s) = \\sum_{a} \\pi(a|s) \\left( R^a_{s} + \\sum_{s'} P^a_{s,s'} V^\\pi_{k}(s')\\right)$\n",
    "- Contraction mapping theorem says that RHS is a contraction map and will converge on repeated iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23315ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MDP\n",
    "S = np.array([0, 2]) \n",
    "A = np.array([100, 200])\n",
    "P = np.array([[[0.5, 0.5],   # P given a=0\n",
    "              [0.8, 0.2]], \n",
    "     \n",
    "             [[0.2, 0.8],   # P given a=0\n",
    "              [0.4, 0.6]]] \n",
    " \n",
    "             )\n",
    "R = np.array([[10, 90], \n",
    "               [5, 30]])\n",
    "d = 0.95\n",
    "s0 = 0\n",
    "pi = np.array([[0.5, 0.5], \n",
    "               [0.5, 0.5]])\n",
    "T = 100\n",
    "\n",
    "def discountedSum(R, d):\n",
    "    sum = 0\n",
    "    for i, r in enumerate(R):\n",
    "        sum += R[i] * (d ** i)\n",
    "    return sum\n",
    "\n",
    "# Histories\n",
    "HS = np.zeros(T+1).astype(int)\n",
    "HR = np.zeros(T).astype(int)\n",
    "HA = np.zeros(T).astype(int)\n",
    "HG = np.zeros(T).astype(int)\n",
    "HV = np.zeros(T).astype(int)\n",
    "HS[0] = s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c8b84d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[673.93939394 716.36363636]\n"
     ]
    }
   ],
   "source": [
    "# Policy Evaluation\n",
    "N = 1000 # iterations\n",
    "V_new = np.zeros(S.shape[0])\n",
    "V_old = np.ones(S.shape[0])\n",
    "for i_n in range(N):\n",
    "    for i_s, s in enumerate(S):\n",
    "        V_new[i_s] = np.dot(pi[i_s], R[:, i_s] + d * np.dot(P[:, i_s, :], V_old))\n",
    "    V_old = V_new.copy()\n",
    "print(V_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d556888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[671.946 707.104]\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo\n",
    "N = 500 \n",
    "V = np.empty((T, S.shape[0]))\n",
    "Q = np.empty((T, A.shape[0], S.shape[0]))\n",
    "G_ = np.empty((T, S.shape[0], N))\n",
    "for i_s, s0 in enumerate(S):\n",
    "    for i_n in range(N):\n",
    "        HS = np.zeros(T+1).astype(int)\n",
    "        HR = np.zeros(T).astype(int)\n",
    "        HA = np.zeros(T).astype(int)\n",
    "        HG = np.zeros(T).astype(int)\n",
    "        HV = np.zeros(T).astype(int)\n",
    "        HS[0] = s0\n",
    "        for t in range(T):\n",
    "            HA[t] = np.random.choice(A, p = pi[np.where(S == HS[t])][0])\n",
    "            HS[t+1] = np.random.choice(S, p = P[np.where(A == HA[t]), np.where(S == HS[t]), :][0][0])\n",
    "            HR[t] = R[np.where(A == HA[t]), np.where(S == HS[t])]\n",
    "        for t in range(0, T, 1):\n",
    "            HG[T-1-t] = discountedSum(HR[T-1-t:T], d)\n",
    "        G_[:, i_s, i_n] = HG\n",
    "    for t in range(T-1):\n",
    "        V[t, i_s] = np.mean(G_[t, i_s, :])           \n",
    "print(V[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234c889",
   "metadata": {},
   "source": [
    "* MDP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda7903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.array([0, 1, 2]) # three states\n",
    "A = np.array([100, 200, 300]) # three decisions\n",
    "P = np.array([[[0.5, 0.4, 0.1],   # P given a=0\n",
    "              [0.8, 0.1, 0.1],\n",
    "              [0.2, 0.1, 0.7]], \n",
    "     \n",
    "             [[0.2, 0.2, 0.6],   # P given a=0\n",
    "              [0.4, 0.3, 0.3],\n",
    "              [0.5, 0.4, 0.1]], \n",
    " \n",
    "             [[0.1, 0.7, 0.2],   # P given a=0\n",
    "              [0.05, 0.9, 0.05],\n",
    "              [0.3, 0.2, 0.5]],\n",
    "             ])\n",
    "R = np.array([[10, 20, 90], \n",
    "               [5, 30, 80],\n",
    "              [4, 4, 110]])\n",
    "d = 0.95\n",
    "s0 = 0\n",
    "pi = np.array([[0.3, 0.5, 0.2], \n",
    "               [0.7, 0.2, 0.1], \n",
    "              [0.3, 0.4, 0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0815bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n",
      "[715.45797971 710.41391389 804.21464968]\n"
     ]
    }
   ],
   "source": [
    "# Policy Evaluation\n",
    "N = 2000\n",
    "V_new = np.zeros(S.shape[0])\n",
    "V_old = np.ones(S.shape[0])\n",
    "print(V_old)\n",
    "for i_n in range(N):\n",
    "    for i_s, s in enumerate(S):\n",
    "        V_new[i_s] = np.dot(pi[i_s], R[:, i_s] + d * np.dot(P[:, i_s, :], V_old))\n",
    "    V_old = V_new.copy()\n",
    "print(V_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba0946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[708.9485 706.222  803.248 ]\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo Check\n",
    "N = 2000 # histories\n",
    "V = np.empty((T, S.shape[0]))\n",
    "Q = np.empty((T, A.shape[0], S.shape[0]))\n",
    "G_ = np.empty((T, S.shape[0], N))\n",
    "for i_s, s0 in enumerate(S):\n",
    "    for i_n in range(N):\n",
    "        HS = np.zeros(T+1).astype(int)\n",
    "        HR = np.zeros(T).astype(int)\n",
    "        HA = np.zeros(T).astype(int)\n",
    "        HG = np.zeros(T).astype(int)\n",
    "        HV = np.zeros(T).astype(int)\n",
    "        HS[0] = s0\n",
    "        for t in range(T):\n",
    "            HA[t] = np.random.choice(A, p = pi[np.where(S == HS[t])][0])\n",
    "            HS[t+1] = np.random.choice(S, p = P[np.where(A == HA[t]), np.where(S == HS[t]), :][0][0])\n",
    "            HR[t] = R[np.where(A == HA[t]), np.where(S == HS[t])]\n",
    "        for t in range(0, T, 1):\n",
    "            HG[T-1-t] = discountedSum(HR[T-1-t:T], d)\n",
    "        G_[:, i_s, i_n] = HG\n",
    "    for t in range(T-1):\n",
    "        V[t, i_s] = np.mean(G_[t, i_s, :])           \n",
    "print(V[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dfd58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
